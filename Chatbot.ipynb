{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e82fb-9b6e-49ab-9d2f-23342be12395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e43f90f-eb00-4b4b-8412-87726c939279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5885bc-8729-4fdc-bd6c-8461325e203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_packages = [\"punkt\", \"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"words\", \"stopwords\"]\n",
    "for pkg in nltk_packages:\n",
    "    try:\n",
    "        nltk.data.find(pkg)\n",
    "    except Exception:\n",
    "        nltk.download(pkg.split('/')[-1] if '/' in pkg else pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f001a596-ca65-46a5-a18b-a9912c4ba583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File paths ---\n",
    "BASE = Path(\".\")\n",
    "INTENTS_FILE = BASE / \"intents.json\"\n",
    "TOKENIZER_FILE = BASE / \"tokenizer.pkl\"\n",
    "MODEL_FILE = BASE / \"intent_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc72142-1538-42dd-894c-7895ab8f3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sample intents (will be written if intents.json missing) ---\n",
    "SAMPLE_INTENTS = {\n",
    "    \"intents\": [\n",
    "        {\n",
    "            \"tag\": \"greeting\",\n",
    "            \"patterns\": [\"Hi\", \"Hey\", \"Hello\", \"Good morning\", \"Good evening\"],\n",
    "            \"responses\": [\"Hello! How can I help you today?\", \"Hi there — what can I do for you?\"]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"goodbye\",\n",
    "            \"patterns\": [\"Bye\", \"See you\", \"Goodbye\", \"I am leaving\"],\n",
    "            \"responses\": [\"Goodbye! Have a great day.\", \"See you later — take care!\"]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"thanks\",\n",
    "            \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\", \"Thanks a lot\"],\n",
    "            \"responses\": [\"You're welcome!\", \"Happy to help!\"]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"hours\",\n",
    "            \"patterns\": [\"What are your hours?\", \"When are you open?\", \"working hours\", \"open time\"],\n",
    "            \"responses\": [\"We are open Monday to Friday, 9am to 6pm.\", \"Our hours are 9:00–18:00, Mon–Fri.\"]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"services\",\n",
    "            \"patterns\": [\"What services do you offer?\", \"Tell me your services\", \"services\"],\n",
    "            \"responses\": [\"We offer A, B and C. Which one are you interested in?\", \"Our main services are: consulting, development, and support.\"]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"fallback\",\n",
    "            \"patterns\": [\"\"],\n",
    "            \"responses\": [\"Sorry, I didn't understand. Can you rephrase?\", \"I don't have an answer for that yet — could you try a different question?\"]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146a3c13-c216-4ce2-a8c5-263ad3041684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utilities ---\n",
    "\n",
    "def ensure_intents_file():\n",
    "    if not INTENTS_FILE.exists():\n",
    "        with open(INTENTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(SAMPLE_INTENTS, f, indent=2)\n",
    "        print(f\"Created sample intents.json at {INTENTS_FILE}\")\n",
    "\n",
    "\n",
    "def load_intents():\n",
    "    with open(INTENTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercase, tokenize, remove stopwords (simple)\n",
    "    tokens = [t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ac1947-99d5-4a89-ae3b-4d23fed3b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare training data for intent classification ---\n",
    "\n",
    "def prepare_training_data(intents, num_words=2000, max_len=20):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    tag_list = []\n",
    "    for intent in intents['intents']:\n",
    "        tag = intent['tag']\n",
    "        if tag not in tag_list:\n",
    "            tag_list.append(tag)\n",
    "        for patt in intent.get('patterns', []):\n",
    "            texts.append(preprocess_text(patt))\n",
    "            labels.append(tag_list.index(tag))\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "    y = tf.keras.utils.to_categorical(labels, num_classes=len(tag_list))\n",
    "\n",
    "    return X, y, tokenizer, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ef2a28-f51b-42c9-8d11-e7b011761a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model building and training ---\n",
    "\n",
    "def build_model(vocab_size, embed_dim, input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=input_length))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_save(intents):\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, tokenizer, tag_list = prepare_training_data(intents)\n",
    "    vocab_size = 2000\n",
    "    embed_dim = 50\n",
    "    max_len = X.shape[1]\n",
    "    model = build_model(vocab_size, embed_dim, max_len, len(tag_list))\n",
    "    print(\"Training model (this may take a moment)...\")\n",
    "    model.fit(X, y, epochs=200, batch_size=8, verbose=0)\n",
    "    model.save(MODEL_FILE)\n",
    "    with open(TOKENIZER_FILE, 'wb') as f:\n",
    "        pickle.dump({'tokenizer': tokenizer, 'max_len': max_len, 'tag_list': tag_list}, f)\n",
    "    print(f\"Saved model to {MODEL_FILE} and tokenizer to {TOKENIZER_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc80450b-c961-4983-a10b-50f4ba8653dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction / NER / Response selection ---\n",
    "\n",
    "def load_resources():\n",
    "    if not MODEL_FILE.exists() or not TOKENIZER_FILE.exists():\n",
    "        return None, None, None\n",
    "    model = load_model(MODEL_FILE)\n",
    "    with open(TOKENIZER_FILE, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    tokenizer = obj['tokenizer']\n",
    "    max_len = obj['max_len']\n",
    "    tag_list = obj['tag_list']\n",
    "    return model, tokenizer, max_len, tag_list\n",
    "\n",
    "\n",
    "def predict_intent(model, tokenizer, max_len, text):\n",
    "    seq = tokenizer.texts_to_sequences([preprocess_text(text)])\n",
    "    seq = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "    probs = model.predict(seq, verbose=0)[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    confidence = float(probs[idx])\n",
    "    return idx, confidence\n",
    "    \n",
    "def extract_entities(text):\n",
    "    # Basic NLTK NER pipeline\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = pos_tag(tokens)\n",
    "    tree = ne_chunk(tags, binary=False)\n",
    "    entities = []\n",
    "    for subtree in tree:\n",
    "        if hasattr(subtree, 'label'):\n",
    "            label = subtree.label()\n",
    "            name = \" \".join([leaf[0] for leaf in subtree.leaves()])\n",
    "            entities.append({'entity': name, 'label': label})\n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_response(intents_data, tag, entities=None):\n",
    "    for intent in intents_data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            resp = random.choice(intent.get('responses', []))\n",
    "            # Simple entity-aware templating: replace {entity} if present\n",
    "            if entities:\n",
    "                # Example: if response contains {entity_name}\n",
    "                for e in entities:\n",
    "                    key = \"{\" + e['label'].lower() + \"}\"\n",
    "                    if key in resp:\n",
    "                        resp = resp.replace(key, e['entity'])\n",
    "            return resp\n",
    "    # fallback\n",
    "    return random.choice(next(i for i in intents_data['intents'] if i['tag']=='fallback')['responses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604ea61f-03b2-4f16-9bb1-cbdc1875ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# --- Flask app ---\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Ensure intents exist\n",
    "ensure_intents_file()\n",
    "intents_data = load_intents()\n",
    "\n",
    "# Train model if missing\n",
    "if not MODEL_FILE.exists() or not TOKENIZER_FILE.exists():\n",
    "    train_and_save(intents_data)\n",
    "\n",
    "# Load resources now\n",
    "MODEL, TOKENIZER, MAX_LEN, TAG_LIST = load_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13b0cb-e081-4ecc-9b6e-c059c4fbc90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [03/Dec/2025 18:34:26] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [03/Dec/2025 18:34:27] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    if not data or 'message' not in data:\n",
    "        return jsonify({'error': 'Please send JSON with a \"message\" field.'}), 400\n",
    "    message = data['message']\n",
    "    # Predict intent\n",
    "    idx, confidence = predict_intent(MODEL, TOKENIZER, MAX_LEN, message)\n",
    "    predicted_tag = TAG_LIST[idx]\n",
    "    # Extract entities\n",
    "    entities = extract_entities(message)\n",
    "    # Get response\n",
    "    response = get_response(intents_data, predicted_tag, entities)\n",
    "    return jsonify({\n",
    "        'message': message,\n",
    "        'intent': predicted_tag,\n",
    "        'confidence': round(confidence, 3),\n",
    "        'entities': entities,\n",
    "        'response': response\n",
    "    })\n",
    "\n",
    "    \n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Chatbot server running. POST JSON {\\\"message\\\": \\\"hello\\\"} to /chat\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start Flask app\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01942640-2370-4f36-b43c-9820d60af360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183fad5-40d7-4b1d-b016-3b2ff24cea6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
